{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Download dataset**\n",
        "**Note:** If you can't download using gdown due to limited number of downloads, please download it manually and upload it to your drive, then copy it from the drive to colab.\n",
        "```python\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!cp /path/to/dataset/on/your/drive .\n",
        "```"
      ],
      "metadata": {
        "id": "y8m6nlphiBzC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZz-y60IxCk3",
        "outputId": "2c9c4289-4217-4ffe-e624-98946e656072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kU64ckjel-CpAH352bwU5hWMFT_caZxZ\n",
            "To: /content/card_fraud_detection.zip\n",
            "100% 69.2M/69.2M [00:01<00:00, 39.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "# https://drive.google.com/file/d/1kU64ckjel-CpAH352bwU5hWMFT_caZxZ/view?usp=drive_link\n",
        "!gdown --id 1kU64ckjel-CpAH352bwU5hWMFT_caZxZ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'card_fraud_detection.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVEUMTb3xV_y",
        "outputId": "92f25348-de1d-4f5a-9318-3a9cbc27c736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  card_fraud_detection.zip\n",
            "  inflating: creditcard.csv          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import libraries**"
      ],
      "metadata": {
        "id": "3W4iguWIxMtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "rjjCPEc8j4uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Read dataset**"
      ],
      "metadata": {
        "id": "7XLKcf8J2vdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'creditcard.csv'\n",
        "df = pd.read_csv(\n",
        "    dataset_path\n",
        ")\n",
        "df"
      ],
      "metadata": {
        "id": "iBQShVY6j8CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "aDUcFLomj-ax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "0Zr5C14Ij_S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Add bias term**"
      ],
      "metadata": {
        "id": "CXfvmGsUkPDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intercept = np.ones((X.shape[0], 1))"
      ],
      "metadata": {
        "id": "Dez4PejmYjQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_b = np.concatenate((intercept, X), axis=1)"
      ],
      "metadata": {
        "id": "fCfXFEBaYjSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. One-hot encoding label**"
      ],
      "metadata": {
        "id": "PcL-BFI-kXsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = np.unique(y, axis=0).shape[0]\n",
        "n_samples = y.shape[0]\n",
        "y_encoded = np.array([np.zeros (n_classes) for _ in range(n_samples)])\n",
        "y_encoded[np.arange(n_samples), y] = 1"
      ],
      "metadata": {
        "id": "xlpGUyE9YkkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Split train, val, test set**"
      ],
      "metadata": {
        "id": "NYM_2Ke2kbWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "random_state = 2\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_b, y_encoded,\n",
        "    test_size=val_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=test_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ],
      "metadata": {
        "id": "MJq5icI3kcVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training samples: {X_train.shape[0]}')\n",
        "print(f'Number of val samples: {X_val.shape[0]}')\n",
        "print(f'Number of test samples: {X_test.shape[0]}')"
      ],
      "metadata": {
        "id": "LK7zXWY_kgRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Normalization**"
      ],
      "metadata": {
        "id": "IxT04b28kiLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = StandardScaler()\n",
        "X_train[:, 1:] = normalizer.fit_transform(X_train[:, 1:])\n",
        "X_val[:, 1:] = normalizer.transform(X_val[:, 1:])\n",
        "X_test[:, 1:] = normalizer.transform(X_test[:, 1:])"
      ],
      "metadata": {
        "id": "dPzWAiRCki5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "5BvIjPJ1kjrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Define essential functions**"
      ],
      "metadata": {
        "id": "EML1Y1qBkknT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.1. Softmax Function**"
      ],
      "metadata": {
        "id": "HFDzq7xYkoI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    y_hat = np.exp(z) / np.exp(z).sum(axis=1)[:, None]\n",
        "\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "xUXupSHvklv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.2. Hypothesis function**"
      ],
      "metadata": {
        "id": "BAr7qJ94kpXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, theta):\n",
        "    z = np.dot(X, theta)\n",
        "    y_hat = softmax(z)\n",
        "\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "0DrA5LawknTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.3. Cross-entropy loss function**"
      ],
      "metadata": {
        "id": "PlG61z0QkrXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(y_hat, y):\n",
        "    loss = -1 * (np.dot(y.T, np.log(y_hat))) / y.size\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Qf3jm2wJkpzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.4. Gradient function**"
      ],
      "metadata": {
        "id": "QM6OMFMEktm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, y_hat):\n",
        "    dtheta = np.dot(X.T, (y_hat - y)) / y.size\n",
        "\n",
        "    return dtheta"
      ],
      "metadata": {
        "id": "SLUPEWZRkrxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.5. Update weights function**"
      ],
      "metadata": {
        "id": "MdCKbDIEkvAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_theta(theta, gradient, lr):\n",
        "    theta = theta - lr * gradient\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "xKmA64Bekvse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7.6. Accuracy function**"
      ],
      "metadata": {
        "id": "VWgmWHJVkwkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(X, y, theta):\n",
        "    y_hat = predict(X, theta)\n",
        "    acc = (np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)).mean()\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "oT-7DgCCkxbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Training**"
      ],
      "metadata": {
        "id": "h2UzGasSk1uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "epochs = 30\n",
        "batch_size = 1024\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "np.random.seed(random_state)\n",
        "theta = np.random.uniform(\n",
        "    size=(n_features, n_classes)\n",
        ")"
      ],
      "metadata": {
        "id": "MRTGF_Q2k2ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accs = []\n",
        "train_losses = []\n",
        "val_accs = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_batch_losses = []\n",
        "    train_batch_accs = []\n",
        "    val_batch_losses = []\n",
        "    val_batch_accs = []\n",
        "\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "\n",
        "        X_i = X_train[i: i + batch_size]\n",
        "        y_i = y_train[i: i + batch_size]\n",
        "\n",
        "        y_hat = predict(X_i, theta)\n",
        "        train_loss = compute_loss(y_hat, y_i)\n",
        "        dtheta = compute_gradient(X_i, y_i, y_hat)\n",
        "        theta = update_theta(theta, dtheta, lr)\n",
        "        train_batch_losses.append(train_loss)\n",
        "        train_acc = compute_accuracy(X_train, theta, y_train)\n",
        "        train_batch_accs.append(train_acc)\n",
        "\n",
        "        y_val_hat = predict(X_val, theta)\n",
        "        val_loss = compute_loss(y_val_hat, y_val)\n",
        "        val_batch_losses.append(val_loss)\n",
        "        val_acc = compute_accuracy(X_val, theta, y_val)\n",
        "        val_batch_accs.append(val_acc)\n",
        "\n",
        "    train_batch_loss = sum(train_batch_losses) / len(train_batch_losses)\n",
        "    val_batch_loss = sum(val_batch_losses) / len(val_batch_losses)\n",
        "    train_batch_acc = sum(train_batch_accs) / len(train_batch_accs)\n",
        "    val_batch_acc = sum(val_batch_accs) / len(val_batch_accs)\n",
        "\n",
        "    train_losses.append(train_batch_loss)\n",
        "    train_accs.append(train_batch_acc)\n",
        "    val_losses.append(val_batch_loss)\n",
        "    val_accs.append(val_batch_acc)\n",
        "\n",
        "    print(f'\\nEPOCH {epoch + 1}:\\tTraining Loss: {train_batch_loss:.3f}\\tValidation Loss: {val_batch_loss:.3f}')"
      ],
      "metadata": {
        "id": "aUpqHZM5YTAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "ax[0, 0].plot(train_losses, color='green')\n",
        "ax[0, 0].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "\n",
        "ax[0, 1].plot(val_losses, color='orange')\n",
        "ax[0, 1].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 1].set_title('Validation Loss')\n",
        "\n",
        "ax[1, 0].plot(train_accs, color='green')\n",
        "ax[1, 0].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "\n",
        "ax[1, 1].plot(val_accs, color='orange')\n",
        "ax[1, 1].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 1].set_title('Validation Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zPY17Pcsk72t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Evaluation**"
      ],
      "metadata": {
        "id": "woxqkbFuk901"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Val set\n",
        "val_set_acc = compute_accuracy(X_val, y_val, theta)\n",
        "print('Evaluation on validation set:')\n",
        "print(f'Accuracy: {val_set_acc}')"
      ],
      "metadata": {
        "id": "L8YNxEuik_AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set\n",
        "test_set_acc = compute_accuracy(X_test, y_test, theta)\n",
        "print('Evaluation on test set:')\n",
        "print(f'Accuracy: {test_set_acc}')"
      ],
      "metadata": {
        "id": "wp-yv8rZk_uo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}