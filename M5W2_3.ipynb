{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **0. Download dataset**\n",
        "**Note:** If you can't download using gdown due to limited number of downloads, please download it manually and upload it to your drive, then copy it from the drive to colab.\n",
        "```python\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!cp /path/to/dataset/on/your/drive .\n",
        "```"
      ],
      "metadata": {
        "id": "ECrVwhHTSJRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1e1uIwcJ1-MviSn9yk_ldPGffDWVp6yK_/view?usp=drive_link\n",
        "!gdown --id 1e1uIwcJ1-MviSn9yk_ldPGffDWVp6yK_"
      ],
      "metadata": {
        "id": "r7aN3wiivdUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip twitter_sentiment_analysis_3cls_dataset.zip"
      ],
      "metadata": {
        "id": "jkru2bLe4l01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import libraries**"
      ],
      "metadata": {
        "id": "9govEkfdTJVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "IAvQLqFPTHr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Read dataset**"
      ],
      "metadata": {
        "id": "JuyqHDI8TVf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'Twitter_Data.csv'\n",
        "df = pd.read_csv(\n",
        "    dataset_path\n",
        ")\n",
        "df"
      ],
      "metadata": {
        "id": "i0uWI9h4TXuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "6UHefku4Ujgu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "1DVd0gIgUjx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Drop missing value**"
      ],
      "metadata": {
        "id": "8ej3aM_XI-Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "null_rows = df.isnull().any(axis=1)\n",
        "df[null_rows]"
      ],
      "metadata": {
        "id": "-fWdmBs-JDkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "CzLyWoryI_0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "nRHyuSBuJBlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Preprocessing data**\n",
        "\n"
      ],
      "metadata": {
        "id": "sVHQuLtbAv9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_normalize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'^rt[\\s]+', '', text)\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "45GNGGYaAxt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_text'] = df['clean_text'].apply(\n",
        "    lambda x: text_normalize(x)\n",
        ")"
      ],
      "metadata": {
        "id": "yCT-1Bh7Tjhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "FI5FaYVi-4hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. One-hot encoding label**"
      ],
      "metadata": {
        "id": "eqLmhRMGBVJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = df['category'].nunique()\n",
        "n_samples = df['category'].size\n",
        "\n",
        "y = df['category'].to_numpy() + 1\n",
        "y = y,astype(np.uint8)\n",
        "\n",
        "y_encoded = np.array([np.zeros (n_classes) for _ in range(n_samples)])\n",
        "y_encoded[np.arange(n_samples), y] = 1"
      ],
      "metadata": {
        "id": "A7n8OkbyBXpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Create train, val, test set**"
      ],
      "metadata": {
        "id": "Oap8OOdJThGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y_encoded, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "BLOSJsFvcKFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "random_state = 2\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=val_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=test_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ],
      "metadata": {
        "id": "UppH-uK0UROv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of training samples: {X_train.shape[0]}')\n",
        "print(f'Number of val samples: {X_val.shape[0]}')\n",
        "print(f'Number of test samples: {X_test.shape[0]}')"
      ],
      "metadata": {
        "id": "wrZklXUWUZX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Define Softmax Regression model**"
      ],
      "metadata": {
        "id": "ElXzsGn6adqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxRegression(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(SoftmaxRegression, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "W6l_5GZ4agfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(y_hat, y_true):\n",
        "    y_hat = predict(X, theta)\n",
        "    acc = (torch.argmax(y_hat, axis=1) == torch.argmax(y, axis=1)).mean()\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "Q3Aj-Lb2d1SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Training**"
      ],
      "metadata": {
        "id": "vjTzJ0HtWSMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "epochs = 500\n",
        "torch.manual_seed(random_state)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_state)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = y_train.shape[1]\n",
        "\n",
        "model = SoftmaxRegression(\n",
        "    input_dim, output_dim\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(\n",
        "    model.parameters(), lr=lr\n",
        ")"
      ],
      "metadata": {
        "id": "-vm95Cm9WfnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_accs = []\n",
        "train_losses = []\n",
        "val_accs = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in epochs:\n",
        "    model.train()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_hat = model(X_train)\n",
        "\n",
        "    train_loss = criterion(y_hat, y_train)\n",
        "    train_acc = compute_accuracy(y_hat, y_train)\n",
        "    train_losses.append(train_loss.item())\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_val_hat = model(X_val)\n",
        "        val_loss = criterion(y_val_hat, y_val)\n",
        "        val_losses.append(val_loss.item())\n",
        "        val_acc = compute_accuracy(y_val_hat, y_val)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "    print(f'\\nEPOCH {epoch + 1}:\\tTraining Loss: {train_batch_loss:.3f}\\tValidation Loss: {val_batch_loss:.3f}')"
      ],
      "metadata": {
        "id": "7r_E1O9u12xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
        "ax[0, 0].plot(train_losses, color='green')\n",
        "ax[0, 0].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 0].set_title('Training Loss')\n",
        "\n",
        "ax[0, 1].plot(val_losses, color='orange')\n",
        "ax[0, 1].set(xlabel='Epoch', ylabel='Loss')\n",
        "ax[0, 1].set_title('Validation Loss')\n",
        "\n",
        "ax[1, 0].plot(train_accs, color='green')\n",
        "ax[1, 0].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 0].set_title('Training Accuracy')\n",
        "\n",
        "ax[1, 1].plot(val_accs, color='orange')\n",
        "ax[1, 1].set(xlabel='Epoch', ylabel='Accuracy')\n",
        "ax[1, 1].set_title('Validation Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g78-CtUavsBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Evaluation**"
      ],
      "metadata": {
        "id": "4YpiJSA-WdnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Val set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat = model(X_val)\n",
        "    val_set_acc = compute_accuracy(y_hat, y_val)\n",
        "    print('Evaluation on validation set:')\n",
        "    print(f'Accuracy: {val_set_acc}')"
      ],
      "metadata": {
        "id": "Osq7IQtj2e3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_hat = model(X_test)\n",
        "    test_set_acc = compute_accuracy(y_hat, y_test)\n",
        "    print('Evaluation on test set:')\n",
        "    print(f'Accuracy: {test_set_acc}')"
      ],
      "metadata": {
        "id": "gbwp1s7M2gIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}